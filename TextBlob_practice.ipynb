{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdffWe-nFAjn",
        "outputId": "cbf5cfaf-47b7-47df-82f3-38b7b9f232fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.65.0)\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.5.7)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=b75817fc08a02832aa9ece371df0c20b8e98dda06e8627e8dd550bb3235b46b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 4.0.0\n",
            "    Uninstalling chardet-4.0.0:\n",
            "      Successfully uninstalled chardet-4.0.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install textblob\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('popular')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkCMvMyIFISp",
        "outputId": "898b4e36-09b4-404c-c756-3c8b388abc93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Textblob**"
      ],
      "metadata": {
        "id": "e93LDjHEFZ8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Language detection**\n",
        "1. With the help of google translate,textblob detects the language of input text.\n",
        "2. Textblob is also able to translate text from one language to another language."
      ],
      "metadata": {
        "id": "jw-MPXi2FrUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator\n",
        "\n",
        "# Create a translator object\n",
        "translator = Translator()\n",
        "\n",
        "# Define the input text\n",
        "input_text = \"Hey XYZ, How are You\"\n",
        "\n",
        "# Detect the language of the input text\n",
        "detected_lang = translator.detect(input_text).lang\n",
        "\n",
        "print(\"Detected Language is:\", detected_lang)\n",
        "\n",
        "# Translate the input text to Gujarati\n",
        "translated_text = translator.translate(input_text, dest='gu').text\n",
        "\n",
        "print(\"Translated text in Gujarati:\", translated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAH-wj55JCLp",
        "outputId": "79ea78b5-7870-4a32-c40c-964fc00b44fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Language is: en\n",
            "Translated text in Gujarati: હે xyz, તમે કેમ છો\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spelling Correction**"
      ],
      "metadata": {
        "id": "fsli6NLQCHi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "text=\"\"\" ABCD Corp alays values ttheir employees!!! \"\"\""
      ],
      "metadata": {
        "id": "V6hqJZJ3BfjC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWQsXkQ8Cdrk",
        "outputId": "fa5c38ac-278b-43a4-9b8a-3d4f3d70f0b2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ABCD Corp alays values ttheir employees!!! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob=TextBlob(text)"
      ],
      "metadata": {
        "id": "VgDOZW0gCejQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz7Lwh4XCsPk",
        "outputId": "bf629e24-25ed-4720-b4b9-8dac6e82543a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\" ABCD Corp alays values ttheir employees!!! \")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob.correct()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLCvvd7yChNa",
        "outputId": "0f66b636-7f1a-4260-a4e8-d4629db70aee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\" ABCD For always values their employees!!! \")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TextBlob('hassss').correct()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_9b6weoCi9y",
        "outputId": "99716f5e-9784-43b5-c674-362d26296cac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"masses\")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Count**\n",
        "With the help of word count , we can count the frequency of words or a noun phrase in a given sentence."
      ],
      "metadata": {
        "id": "As_GjMdAC3SL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Sentiment Analysis is a process by which we can find the sentiment of a text.Sentiment can be positive ,Negative or Neutral \""
      ],
      "metadata": {
        "id": "jQ0DZ0KfCz0g"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blob=TextBlob(text)"
      ],
      "metadata": {
        "id": "KPtCihCfDSsR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blob.word_counts[\"analysis\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDF4T5hDDa0a",
        "outputId": "52636b55-c5d6-4a7c-d06f-d4d178657d66"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob.word_counts[\"Analysis\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvQF37sXDkar",
        "outputId": "24809cd6-910c-47b3-fd8e-ab5771d5090e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob.word_counts[\"Sentiment\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zobdkR8DnVg",
        "outputId": "779eb9f4-bb62-4b28-eb0c-1136cb26ef4c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob.word_counts[\"sentiment\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt-0aKPlDpC-",
        "outputId": "fb6a973b-1a72-478b-ba3f-58af7c86b8d9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **POS Tagging**\n",
        "With the help of tags function of textblobs , we can get tage each words of a sentence with a tag that can be either noun ,pronoun,verb,adverb,adjective and more."
      ],
      "metadata": {
        "id": "NUzC7JI-Dzrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "text=TextBlob(\"My name is XYZ. I like to read about NLP. I work at ABCD corp.\")\n",
        "print(text.tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfGakONaDq7H",
        "outputId": "bccde745-14e0-402b-8664-3492ec4874ac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('XYZ', 'NNP'), ('I', 'PRP'), ('like', 'VBP'), ('to', 'TO'), ('read', 'VB'), ('about', 'IN'), ('NLP', 'NNP'), ('I', 'PRP'), ('work', 'VBP'), ('at', 'IN'), ('ABCD', 'NNP'), ('corp', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_tuple=[]\n",
        "for i in text.tags:\n",
        "  print(i)\n",
        "  if 'VBP' not in i[1]:\n",
        "    new_tuple.append(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6h_o07VEWej",
        "outputId": "5c6df4b4-0818-4245-9d58-8f16d5fd73c1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('My', 'PRP$')\n",
            "('name', 'NN')\n",
            "('is', 'VBZ')\n",
            "('XYZ', 'NNP')\n",
            "('I', 'PRP')\n",
            "('like', 'VBP')\n",
            "('to', 'TO')\n",
            "('read', 'VB')\n",
            "('about', 'IN')\n",
            "('NLP', 'NNP')\n",
            "('I', 'PRP')\n",
            "('work', 'VBP')\n",
            "('at', 'IN')\n",
            "('ABCD', 'NNP')\n",
            "('corp', 'NN')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_tuple"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlLs70miE2Zi",
        "outputId": "1272a9ea-73f7-4bac-a7fd-324a81fa0fcb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('My', 'PRP$'),\n",
              " ('name', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('XYZ', 'NNP'),\n",
              " ('I', 'PRP'),\n",
              " ('to', 'TO'),\n",
              " ('read', 'VB'),\n",
              " ('about', 'IN'),\n",
              " ('NLP', 'NNP'),\n",
              " ('I', 'PRP'),\n",
              " ('at', 'IN'),\n",
              " ('ABCD', 'NNP'),\n",
              " ('corp', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "value=''\n",
        "for i in new_tuple:\n",
        "  value=value+\" \"+\"\".join(i[0])"
      ],
      "metadata": {
        "id": "OTczmKCyE-Dl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YFIwVyYMFOe-",
        "outputId": "c60f1b86-2c35-46fd-d7a3-a9cfc96c4d9a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' My name is XYZ I to read about NLP I at ABCD corp'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**\n",
        "\n",
        "\n",
        "*   Corpus (or corpora in plural)-Corpus is nothing but a collection of text data.Text maybe in one or combination of two or  more language.\n",
        "*   Token- The term token is nothing but the total number of words in a text ,corpus,etc, regardless of their frequency of occurence in the text.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "67xqkebYFs9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\"\n",
        "R is  a comprehensive statistical and graphical programming language, which is fast gaining popularity among data analysts . One of R’s strengths is the ease with which well-designed publication-quality plots can be produced, including mathematical symbols and formulae where needed. Great care has been taken over the defaults for the minor design choices in graphics, but the user retains full control.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zh6BS6MNFPDi"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blob_object=TextBlob(text)"
      ],
      "metadata": {
        "id": "UtpHgCLzGz66"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Word tokenization of the sample corpus\n",
        "corpus_words=blob_object.words"
      ],
      "metadata": {
        "id": "qqbJbGAYG_VE"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObkNRyN4HJju",
        "outputId": "bf82b045-e3f5-4faf-f4b9-83adfbc94a21"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['R', 'is', 'a', 'comprehensive', 'statistical', 'and', 'graphical', 'programming', 'language', 'which', 'is', 'fast', 'gaining', 'popularity', 'among', 'data', 'analysts', 'One', 'of', 'R', '’', 's', 'strengths', 'is', 'the', 'ease', 'with', 'which', 'well-designed', 'publication-quality', 'plots', 'can', 'be', 'produced', 'including', 'mathematical', 'symbols', 'and', 'formulae', 'where', 'needed', 'Great', 'care', 'has', 'been', 'taken', 'over', 'the', 'defaults', 'for', 'the', 'minor', 'design', 'choices', 'in', 'graphics', 'but', 'the', 'user', 'retains', 'full', 'control'])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(corpus_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbYpEOaPHNJS",
        "outputId": "df348554-4569-4a9c-a284-4f7a9b437622"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_sentence=blob_object.sentences"
      ],
      "metadata": {
        "id": "zYJGQ2aAHfVQ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCoU49riHo9n",
        "outputId": "4ebdf3f6-ce38-40f2-9d46-8f28a189c2d8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sentence(\"\n",
              " R is  a comprehensive statistical and graphical programming language, which is fast gaining popularity among data analysts .\"),\n",
              " Sentence(\"One of R’s strengths is the ease with which well-designed publication-quality plots can be produced, including mathematical symbols and formulae where needed.\"),\n",
              " Sentence(\"Great care has been taken over the defaults for the minor design choices in graphics, but the user retains full control.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(corpus_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O7haijLHqKt",
        "outputId": "75ae0901-e1f9-4532-ed69-12ad69207c7d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pluralization of words using Textblob**"
      ],
      "metadata": {
        "id": "wXAm4iU8H56a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import Word\n",
        "w=Word('Platform')\n",
        "w.pluralize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JkgPe1a-H4ai",
        "outputId": "42b473bb-5af6-4b57-d74f-fc2ac965f040"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Platforms'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import Word\n",
        "w=Word('Platforms')\n",
        "w.pluralize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vBz0nlocIMhG",
        "outputId": "5168e266-c479-41db-ae15-cc29ccb7aa35"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Platformss'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob=TextBlob(\"Great Learning is great platform to learn data science. \\n It helps community through blogs, Youtube, GLA ,etc .\")\n",
        "for word,pos in blob.tags:\n",
        "  if pos=='NN':\n",
        "    print(word.pluralize())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zVBF7wQIQKX",
        "outputId": "cc92c7ba-ccfd-4e36-c429-46997fdcfafb"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "platforms\n",
            "sciences\n",
            "communities\n",
            "etcs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization using TextBlob**"
      ],
      "metadata": {
        "id": "TwEt-q0XI8Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blob=TextBlob(\"Great Learning is great platform to learn data science. \\n It helps community through blogs, Youtube, GLA ,etc .\")\n",
        "words=blob.words\n",
        "for word in words:\n",
        "  print(\"Original : \",word,\"| LEMMA: \",word.lemmatize(), \"| STEM: \",word.stem())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlVEWXIUIs3h",
        "outputId": "46136a92-89f3-48eb-8030-cfc33a965b11"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original :  Great | LEMMA:  Great | STEM:  great\n",
            "Original :  Learning | LEMMA:  Learning | STEM:  learn\n",
            "Original :  is | LEMMA:  is | STEM:  is\n",
            "Original :  great | LEMMA:  great | STEM:  great\n",
            "Original :  platform | LEMMA:  platform | STEM:  platform\n",
            "Original :  to | LEMMA:  to | STEM:  to\n",
            "Original :  learn | LEMMA:  learn | STEM:  learn\n",
            "Original :  data | LEMMA:  data | STEM:  data\n",
            "Original :  science | LEMMA:  science | STEM:  scienc\n",
            "Original :  It | LEMMA:  It | STEM:  it\n",
            "Original :  helps | LEMMA:  help | STEM:  help\n",
            "Original :  community | LEMMA:  community | STEM:  commun\n",
            "Original :  through | LEMMA:  through | STEM:  through\n",
            "Original :  blogs | LEMMA:  blog | STEM:  blog\n",
            "Original :  Youtube | LEMMA:  Youtube | STEM:  youtub\n",
            "Original :  GLA | LEMMA:  GLA | STEM:  gla\n",
            "Original :  etc | LEMMA:  etc | STEM:  etc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w=Word('learning')\n",
        "w.lemmatize(\"n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ARrhEq1aJT-P",
        "outputId": "8d007fd3-ab39-4e3d-d305-b2e1a2a54f0a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'learning'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w=Word('learning')\n",
        "w.lemmatize(\"v\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XzfDBX_FJlb_",
        "outputId": "83412401-da34-4d30-8136-26e01096086e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'learn'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N-gram in Textblob**"
      ],
      "metadata": {
        "id": "0Fir4McmJtgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb0wIJFWJqtM",
        "outputId": "8846e020-b8ba-4eb3-b047-08ca8387fcf2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"Great Learning is great platform to learn data science. \n",
              " It helps community through blogs, Youtube, GLA ,etc .\")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob.ngrams(n=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltcmiI7CJzL1",
        "outputId": "74616c70-af01-4f52-b32b-824a66deebe9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['Great']),\n",
              " WordList(['Learning']),\n",
              " WordList(['is']),\n",
              " WordList(['great']),\n",
              " WordList(['platform']),\n",
              " WordList(['to']),\n",
              " WordList(['learn']),\n",
              " WordList(['data']),\n",
              " WordList(['science']),\n",
              " WordList(['It']),\n",
              " WordList(['helps']),\n",
              " WordList(['community']),\n",
              " WordList(['through']),\n",
              " WordList(['blogs']),\n",
              " WordList(['Youtube']),\n",
              " WordList(['GLA']),\n",
              " WordList(['etc'])]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob.ngrams(n=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR6sGva7J5_e",
        "outputId": "14ecd119-2e37-46f0-eb69-bd59513a1ace"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['Great', 'Learning']),\n",
              " WordList(['Learning', 'is']),\n",
              " WordList(['is', 'great']),\n",
              " WordList(['great', 'platform']),\n",
              " WordList(['platform', 'to']),\n",
              " WordList(['to', 'learn']),\n",
              " WordList(['learn', 'data']),\n",
              " WordList(['data', 'science']),\n",
              " WordList(['science', 'It']),\n",
              " WordList(['It', 'helps']),\n",
              " WordList(['helps', 'community']),\n",
              " WordList(['community', 'through']),\n",
              " WordList(['through', 'blogs']),\n",
              " WordList(['blogs', 'Youtube']),\n",
              " WordList(['Youtube', 'GLA']),\n",
              " WordList(['GLA', 'etc'])]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob.ngrams(n=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGRLcKiwJ7Rv",
        "outputId": "d9054a6d-6242-4ec1-c1d9-1bd006c707d3"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['Great', 'Learning', 'is']),\n",
              " WordList(['Learning', 'is', 'great']),\n",
              " WordList(['is', 'great', 'platform']),\n",
              " WordList(['great', 'platform', 'to']),\n",
              " WordList(['platform', 'to', 'learn']),\n",
              " WordList(['to', 'learn', 'data']),\n",
              " WordList(['learn', 'data', 'science']),\n",
              " WordList(['data', 'science', 'It']),\n",
              " WordList(['science', 'It', 'helps']),\n",
              " WordList(['It', 'helps', 'community']),\n",
              " WordList(['helps', 'community', 'through']),\n",
              " WordList(['community', 'through', 'blogs']),\n",
              " WordList(['through', 'blogs', 'Youtube']),\n",
              " WordList(['blogs', 'Youtube', 'GLA']),\n",
              " WordList(['Youtube', 'GLA', 'etc'])]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zjk42zVHJ8dg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}